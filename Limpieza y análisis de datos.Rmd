---
title: "PRACTICA 2: LIMPIEZA Y VALIDACIÓN DE LOS DATOS"
author: "Gines Molina e Iñigo Alvarez"
date: "29/12/2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r 1, message=FALSE, warning=FALSE}
#cargamos los paquetes que voy a usar
library(dplyr)
```

# 1. Descripción del dataset

El dataset es un listado de ofertas de trabajo publicadas en Linkedin para ciencia de datos (térnimo "Data Scientist") que fue obtenido en la primera parte de esta práctica.

Consta de 6 archivos CSV con las ofertas encontradas a nivel de Escocia, Reino Unido, España, Mundo y remoto.

# 2. Integración y selección de los datos de interés a analizar

El primer paso es integrar los distintos archivos en un mismo dataset.

```{r}
Scotland <- read.csv("Scotland.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
Spain <- read.csv("Spain.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
UK <- read.csv("UK.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
WorldWide <- read.csv("WorldWide.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
Remote <- read.csv("Remote.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
jobs <- rbind(Scotland, Spain, UK, WorldWide, Remote)
```

```{r}
str(jobs)
```

Quitamos la primera columna ya que no es más que un índice que no aporta ninguna información.

```{r}
jobs <- jobs[2:17]
```

# 3. Limpieza de los datos

Al haber integrado varios archivos de búsquedas de ámbitos territoriales que se solapan lo primero es eliminar las posibles duplicidades que podamos encontrar.

```{r}
sum(duplicated(jobs))
```

Hay `r sum(duplicated(wines))` observaciones duplicadas así que las eliminamos.

```{r}
jobs <- distinct(jobs)
```

# 3.1. Valores perdidos

Buscamos valores perdidos.
```{r}
sum(is.na(jobs))
sum(jobs=="")
sum(jobs=="None")
```

Suistituimos los valores en blanco por NA.

```{r}
jobs[jobs==""] <-- NA
jobs[jobs=="None"] <-- NA
```

Para corrgir los tipos de datos vamos a ver qué variables se podrían convertir a tipo factor.
```{r}
sapply(jobs, function(x) length(unique(x)))
```


Corregimos los tipos de los datos.
```{r}
jobs$Date <- as.Date(jobs$Date, format="%Y-%m-%d")
jobs$Solicitudes <- as.numeric(jobs$Solicitudes)
jobs$Visualizaciones <- as.numeric(jobs$Visualizaciones)
jobs$Level <- as.factor(jobs$Level)
jobs$Type <- as.factor(jobs$Type)
jobs$Empleados <- as.factor(jobs$Empleados)
jobs$Quick.Application <- as.factor(jobs$Quick.Application)
jobs$Recommended.Flavor <- as.factor(jobs$Recommended.Flavor)
```

Mostramos la distribución de los valores perdidos.
```{r}
library(VIM)
aggr(jobs, numbers=TRUE, sortVars=TRUE, labels=names(jobs),
cex.axis=.7, gap=3, ylab=c("Missing data","Pattern"))
```

```{r}
summary(jobs)
```

```{r}
boxplot(jobs$Solicitudes)
boxplot(jobs$Visualizaciones)
```


```{r}
head(jobs, 10)
```

```{r}
tail(jobs, 10)
```

