---
title: "PRACTICA 2: LIMPIEZA Y VALIDACIÓN DE LOS DATOS"
author: "Gines Molina e Iñigo Alvarez"
date: "29/12/2020"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Hellodsfsdf

asdf
asdf
asd
f

```{r}
# Carga de librerías
library(dplyr)
library(ggplot2)
library(VIM)
library(highcharter)
library(maps)
library(ggmap)
library(ggpubr)
```

# 1. Descripción del dataset

El dataset es un listado de ofertas de trabajo publicadas en Linkedin para ciencia de datos (término "Data Scientist") que fue obtenido en la primera parte de esta práctica.

Consta de 6 archivos CSV con las ofertas encontradas a nivel de Escocia, Reino Unido, España, Mundo y remoto.

# 2. Integración y selección de los datos de interés a analizar

El primer paso es integrar los distintos archivos en un mismo dataset.

```{r}
Scotland <- read.csv("data/Scotland.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
Spain <- read.csv("data/Spain.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
UK <- read.csv("data/UK.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
WorldWide <- read.csv("data/WorldWide.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
Remote <- read.csv("data/Remote.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
jobs <- rbind(Scotland, Spain, UK, WorldWide, Remote)
```

```{r}
str(jobs)
```

Quitamos la primera columna ya que no es más que un índice que no aporta ninguna información.

```{r}
jobs <- jobs[2:17]
```

Cambiamos a numéricas las columnas que deberían ser de este tipo pero se ven como "character".

```{r}

```

Discretización y poner en varias variables con variables binarias (1 y 0)

```{r}

```

# 3. Limpieza de los datos

Al haber integrado varios archivos de búsquedas de ámbitos territoriales que se solapan lo primero es eliminar las posibles duplicidades que podamos encontrar.

```{r}
sum(duplicated(jobs))
```

Hay `r sum(duplicated(jobs))` observaciones duplicadas así que las eliminamos.

```{r}
jobs <- distinct(jobs)
```

# 3.1. Valores perdidos

Buscamos valores perdidos.

```{r}
sum(is.na(jobs))
sum(jobs=="")
sum(jobs=="None")
```

Sustituimos los valores en blanco por NA.

```{r}
jobs[jobs==""] <-- NA
jobs[jobs=="None"] <-- NA
```

Corregimos los tipos de los datos.

```{r}
jobs$Date <- as.Date(jobs$Date, format="%Y-%m-%d")
jobs$Solicitudes <- as.numeric(jobs$Solicitudes)
jobs$Visualizaciones <- as.numeric(jobs$Visualizaciones)
sum(is.na(jobs))
```

Mostramos la distribución de los valores perdidos.

```{r}
aggr(jobs, numbers=TRUE, sortVars=TRUE, labels=names(jobs),
cex.axis=.7, gap=3, ylab=c("Missing data","Pattern"))
```

```{r}
summary(jobs)
```


```{r}
head(jobs, 10)
```

```{r}
tail(jobs, 10)
```

```{r}
world_map <- map_data("world")
ggplot(world_map, aes(x = long, y = lat, group = group)) +
  geom_polygon(fill="lightgray", colour = "white")

world <- map_data("world")
head(world)

locs <- c('Jiron Cuzco 423, Magdalena del Mar', 'Av Nicolas Arriola 500, La Victoria')
geocode(locs)

geocodeAdddress <- function(address) {
  require(RJSONIO)
  url <- "http://maps.google.com/maps/api/geocode/json?address="
  url <- URLencode(paste(url, address, "&sensor=false", sep = ""))
  x <- fromJSON(url, simplify = FALSE)
  if (x$status == "OK") {
    out <- c(x$results[[1]]$geometry$location$lng,
             x$results[[1]]$geometry$location$lat)
  } else {
    out <- NA
  }
  Sys.sleep(0.2)  # API only allows 5 requests per second
  out
}
geocodeAdddress("Time Square, New York City")

# Loop through the addresses to get the latitude and longitude of each address and add it to the
# origAddress data frame in new columns lat and lon
for(i in 1:nrow(jobs)){
  # Print("Working...")
  result <- geocode(jobs$Location[i], output = "latlona", source = "google")
  jobs$lon[i] <- as.numeric(result[1])
  jobs$lat[i] <- as.numeric(result[2])
  jobs$geoAddress[i] <- as.character(result[3])
}
# Write a CSV file containing origAddress to the working directory
write.csv(jobs, "geocoded.csv", row.names=FALSE)

library(tmaptools)
pubs_tmaptools <- geocode_OSM(Scotland$Location, details = TRUE, as.data.frame = TRUE)
#Eliminar si contiene alrededores la localizacion
Spain <- Spain[!grepl("alrededores", Spain[["Location"]]), ]
# Eliminar y alrededores
library(tidyverse)
Spain$Location <- str_remove(Spain$Location, "^*y alrededores.*$")
Spain$Location[which(Spain$Location == "Greater Barcelona Metropolitan Area")] <- "Barcelona"
pubs_tmaptools2 <- geocode_OSM(Spain$Location, details = TRUE, as.data.frame = TRUE)

Scotland<-merge(x=Scotland,y=pubs_tmaptools, by = 0, all= TRUE)
Scotland<-na.omit(Scotland)

library(stringi)
Spain<-merge(x=Spain,y=pubs_tmaptools2, by = 0, all= TRUE)
Spain<-na.omit(Spain)

NI <- map_data("world") %>%
  filter(region == "Spain")

library(plotly)
library(viridis)
p <- Spain %>%
  ggplot() +
    geom_polygon(data = NI, aes(x=long, y = lat, group = group), fill="grey", alpha=0.3) +
    geom_count(aes(x=lon, y=lat)) +
    scale_size_continuous(range=c(1,15)) +
    scale_color_viridis(option="inferno") +
    theme_void() +
    ylim(35,45) +
    coord_map() +
    theme(legend.position = "none")
p

NI <- map_data("world") %>%
  filter(region == "UK")
p <- Scotland %>%
  ggplot() +
    geom_polygon(data = NI, aes(x=long, y = lat, group = group), fill="grey", alpha=0.3) +
    geom_count(aes(x=lon, y=lat)) +
    scale_size_continuous(range=c(1,3)) +
    scale_color_viridis(option="inferno") +
    theme_void() +
    coord_map() +
    theme(legend.position = "none")
p

library(tmaptools)
WorldWide$Location[which(WorldWide$Location == "Cracow, Lesser Poland District, Poland")] <- "Cracow, Poland"
WorldWide$Location[which(WorldWide$Location == "Barcelona y alrededores")] <- "Barcelona, Spain"
WorldWide$Location[which(WorldWide$Location == "New York City Metropolitan Area")] <- "New York"
WorldWide$Location[which(WorldWide$Location == "Hong Kong, Hong Kong SAR")] <- "Hong Kong"
WorldWide$Location[which(WorldWide$Location == "Burnaby (Maywood / Marlborough / Oakalla / Windsor), V5H, CA")] <- "Burnaby"
WorldWide$Location[which(WorldWide$Location == "District Brno-City, Czech Republic")] <- "Czech Republic"
WorldWide$Location[which(WorldWide$Location == "Silkeborg, Middle Jutland, Denmark")] <- "Denmark"
WorldWide$Location[which(WorldWide$Location == "Prague, The Capital, Czech Republic")] <- "Prague"
WorldWide$Location[which(WorldWide$Location == "Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia")] <- "Malaysia"
WorldWide$Location[which(WorldWide$Location == "Genève et périphérie")] <- "Genève"
WorldWide$Location[which(WorldWide$Location == "Dallas-Fort Worth Metroplex")] <- "Dallas"
WorldWide$Location[which(WorldWide$Location == "Raleigh-Durham-Chapel Hill Area")] <- "Raleigh"
WorldWide$Location[which(WorldWide$Location == "Santiago de Compostela y alrededores")] <- "Santiago de Compostela"
WorldWide$Location[which(WorldWide$Location == "Herzliyya, Tel Aviv, Israel")] <- "Herzliya"
WorldWide$Location[which(WorldWide$Location == "New Territories, Hong Kong SAR")] <- "Hong Kong"
# No results found for "Cracow, Lesser Poland District, Poland".
# No results found for "Barcelona y alrededores".
# No results found for "New York City Metropolitan Area".
# No results found for "Hong Kong, Hong Kong SAR".
# No results found for "New York City Metropolitan Area".
# No results found for "Burnaby (Maywood / Marlborough / Oakalla / Windsor), V5H, CA".
# No results found for "District Brno-City, Czech Republic".
# No results found for "New York City Metropolitan Area".
# No results found for "Silkeborg, Middle Jutland, Denmark".
# No results found for "Prague, The Capital, Czech Republic".
# No results found for "Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia".
# No results found for "Genève et périphérie".
# No results found for "Dallas-Fort Worth Metroplex".
# No results found for "Dallas-Fort Worth Metroplex".
# No results found for "Raleigh-Durham-Chapel Hill Area".
# No results found for "Santiago de Compostela y alrededores".
# No results found for "Herzliyya, Tel Aviv, Israel".
# No results found for "New Territories, Hong Kong SAR".
pubs_tmaptools3 <- geocode_OSM(WorldWide$Location, details = TRUE, as.data.frame = TRUE)
WorldWide<-merge(x=WorldWide,y=pubs_tmaptools3, by = 0, all= TRUE)
WorldWide<-na.omit(WorldWide)
NI <- map_data("world")
p <- WorldWide %>%
  ggplot() +
    geom_polygon(data = NI, aes(x=long, y = lat, group = group), fill="grey", alpha=0.3) +
    geom_count(aes(x=lon, y=lat)) +
    scale_size_continuous(range=c(0.01,3)) +
    scale_color_viridis(option="inferno") +
    theme_void() +
    coord_map() +
    theme(legend.position = "none")
p

Remote$Location[which(Remote$Location == "Raleigh-Durham-Chapel Hill Area")] <- "Raleigh"
Remote$Location[which(Remote$Location == "New York City Metropolitan Area")] <- "New York"
Remote$Location[which(Remote$Location == "Des Moines Metropolitan Area")] <- "Des Moines"
Remote$Location[which(Remote$Location == "Greater Minneapolis-St. Paul Area")] <- "Minneapolis"
Remote$Location[which(Remote$Location == "Greater Munich Metropolitan Area")] <- "Munich"
Remote$Location[which(Remote$Location == "Gurgaon Sub-District, Haryana, India")] <- "Gurgaon"
Remote$Location[which(Remote$Location == "Barcelona y alrededores")] <- "Barcelona"
Remote$Location[which(Remote$Location == "Dallas-Fort Worth Metroplex")] <- "Dallas"
Remote$Location[which(Remote$Location == "Village of Mayfield, OH, US")] <- "Cleveland"
Remote$Location[which(Remote$Location == "Kraków i okolice")] <- "Kraków"
# No results found for "New York City Metropolitan Area".
# No results found for "Raleigh-Durham-Chapel Hill Area".
# No results found for "New York City Metropolitan Area".
# No results found for "New York City Metropolitan Area".
# No results found for "New York City Metropolitan Area".
# No results found for "New York City Metropolitan Area".
# No results found for "New York City Metropolitan Area".
# No results found for "New York City Metropolitan Area".
# No results found for "Des Moines Metropolitan Area".
# No results found for "New York City Metropolitan Area".
# No results found for "New York City Metropolitan Area".
# No results found for "Greater Minneapolis-St. Paul Area".
# No results found for "New York City Metropolitan Area".
# No results found for "Greater Munich Metropolitan Area".
# No results found for "New York City Metropolitan Area".
# No results found for "Gurgaon Sub-District, Haryana, India".
# No results found for "Barcelona y alrededores".
# No results found for "New York City Metropolitan Area".
# No results found for "Dallas-Fort Worth Metroplex".
# No results found for "Village of Mayfield, OH, US".
# No results found for "New York City Metropolitan Area".
# No results found for "Dallas-Fort Worth Metroplex".
# No results found for "Greater Minneapolis-St. Paul Area".
# No results found for "New York City Metropolitan Area".
# No results found for "New York City Metropolitan Area".
# No results found for "New York City Metropolitan Area".
# No results found for "Dallas-Fort Worth Metroplex".
# No results found for "New York City Metropolitan Area".
# No results found for "Kraków i okolice".
pubs_tmaptools4 <- geocode_OSM(Remote$Location, details = TRUE, as.data.frame = TRUE)
Remote<-merge(x=Remote,y=pubs_tmaptools4, by = 0, all= TRUE)
Remote<-na.omit(Remote)
NI <- map_data("world")
p <- Remote %>%
  ggplot() +
    geom_polygon(data = NI, aes(x=long, y = lat, group = group), fill="grey", alpha=0.3) +
    geom_count(aes(x=lon, y=lat)) +
    scale_size_continuous(range=c(0.01,3)) +
    scale_color_viridis(option="inferno") +
    theme_void() +
    coord_map() +
    theme(legend.position = "none")
p


```

## 4.2. Comprobación de la normalidad y homogeneidad de la varianza.

En el dataset se obtienen XX variables, XX categóricas y XX numéricas. En este apartado se busca verificar si las variables numéricas (lat, lon, Solicitudes y Visualizaciones) siguen una distribución normal.

Para verificarlo, primero se realizará un estudio visual de la normalidad mediante la gráfica quantile-quantile (Q-Q) que dibuja la correlación entre una muestra dada y la distribución normal. En esta primera gráfica se podrá observar el grado de aproximación o similitud con la línea de referencia de 45 grados. Además, también se realizará el estudio visual sobre el histograma de la variable

```{r}

#TODO HACER PARA CADA DATASET

# Comprobación de la normalidad para Remote

# Esto se debera realizar en apartados anteriores
Remote$Solicitudes <- as.numeric(Remote$Solicitudes)
Remote$Visualizaciones <- as.numeric(Remote$Visualizaciones)
Remote<-na.omit(Remote)
# Nos quedamos solo con la latitud y la longitud. El resto de variables de OSM deberán ser eliminadas en apartados anteriores.
# Tambien se deberia haber mezclado los dataframe de latitud y longitud en el original.

par(mfrow=c(2,2))
for(i in 1:ncol(Remote)) {
  if (is.numeric(Remote[,i])){
    qqnorm(Remote[,i],main = paste("Normal Q-Q Plot for ",colnames(Remote)[i]))
    qqline(Remote[,i],col="red")
    hist(Remote[,i],
      main=paste("Histogram for ", colnames(Remote)[i]),
      xlab=colnames(Remote)[i], freq = FALSE)
  }
}

```

No parece que las gráficas representen que claramente las variables siguen una distribución normal. Para verificarlo, a continuación se realiza la comprobación mediante el test de Shapiro-Wilk y así no dar lugar a errores.

```{r}

for(i in 1:ncol(Remote)) {
  if (is.numeric(Remote[,i])){
    result <- shapiro.test(Remote[,i])
    print(paste("Resultados para: ", colnames(Remote)[i]))
    print(result)
  }
}
```

Se puede observar que a través de los test aplicados a las variables cuantitativas, el p-value es menor que 0.05, por lo que se deberá rechazar la hipótesis nula y aceptar que las variables no siguen una distribución normal. Por lo tanto, se deberán utlizar métodos para el análisis que no supongan que las variables cuantitativas siguen una distribución normal.

No obstante, cuando el número de observaciones es mayor o igual a 30, como es el caso de estas variables cuantitativas y debido al teorema central del límite, se podrán utilizar pruebas paramétricas asumiendo que con un aumento de observaciones, la distribución se volvería normal y en forma de campana. Así las variables se podrían aproximar como una distribución normal de media 0 y desviación estándar 1.

A continuación, se lleva a cabo la comprobación de homogeneidad de las varianzas. Se utilizará el test de Flinge-Killneen que resulta apropiado para variables no paramétricas que no siguen una distribución normal.

```{r}

fligner.test(Visualizaciones ~ lat, Remote)
fligner.test(Visualizaciones ~ lon, Remote)
fligner.test(Solicitudes ~ lat, Remote)
fligner.test(Solicitudes ~ lon, Remote)
fligner.test(Visualizaciones ~ Solicitudes, Remote)

```

Se obtiene como resultado que las variables numéricas no son homogéneas según su varianza ya que en todos estos test se consigue un p-value menor de 0.05. Se deberá tener en cuenta para la aplicación de métodos analíticos que asuman homogeneidad de las varianzas.

## 4.3 Aplicación de pruebas estadísticas para comparar los grupos de datos.

En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc. Aplicar al menos tres métodos de análisis diferentes.

### 4.3.1 Grado de independencia y predictividad sobre la variable "Solicitudes".

```{r}
# Eliminar columnas dataframe

borrar <- c("lat_min","lat_max","lon_min","lon_max","X")
Remote <- Remote[ , !(names(Remote) %in% borrar)]
str(Remote)

corr_matrix <- matrix(nc = 2, nr = 0)
colnames(corr_matrix) <- c("estimate", "p-value")
# Calcular el coeficiente de correlación para cada variable cuantitativa
# con respecto al campo "precio"
Remote[,length(Remote)]
for (i in 1:(ncol(Remote) - 1)) {
  if (is.integer(Remote[,i]) | is.numeric(Remote[,i])) {
    spearman_test = cor.test(Remote[,i],Remote$Visualizaciones,method = "spearman")
    corr_coef = spearman_test$estimate
    p_val = spearman_test$p.value
    # Add row to matrix
    pair = matrix(ncol = 2, nrow = 1)
    pair[1][1] = corr_coef
    pair[2][1] = p_val
    corr_matrix <- rbind(corr_matrix, pair)
    rownames(corr_matrix)[nrow(corr_matrix)] <- colnames(Remote)[i]
  }
}
print(corr_matrix)

anova_fare <- aov(lat ~ Solicitudes, Remote)
summary(anova_fare)

anova_age <- aov(lon ~ Solicitudes, Remote)
summary(anova_age)

```

### 4.3.2 Modelo de regresión lineal

```{r}

remote_lat_positive<-Remote[which(Remote$lat>0),]
ntrain <- nrow(remote_lat_positive)*0.8
ntest <- nrow(remote_lat_positive)*0.2
set.seed(12312)
index_train<-sample(1:nrow(remote_lat_positive),size = ntrain)
train<-remote_lat_positive[index_train,]
test<-remote_lat_positive[-index_train,]
modelo<-lm(formula = Solicitudes ~ lat + lon + Visualizaciones, data=train)
summary(modelo)

prob_sl<-predict(modelo, test, type="response")
mc_sl<-data.frame(
  real=test$Solicitudes,
  predicted= prob_sl,
  dif=ifelse(test$Solicitudes>prob_sl, -prob_sl*100/test$Solicitudes,prob_sl*100/test$Solicitudes)
  )
colnames(mc_sl)<-c("Real","Predecido","Dif%")
knitr::kable(mc_sl)

```


```{r}
library(magrittr)
Remote %<>%
  mutate(Date= as.Date(Date, format= "%Y-%m-%d"))
Remote$Solicitudes <- as.numeric(Remote$Solicitudes)
Remote$Visualizaciones <- as.numeric(Remote$Visualizaciones)
WorldWide %<>%
  mutate(Date= as.Date(Date, format= "%Y-%m-%d"))
UK %<>%
  mutate(Date= as.Date(Date, format= "%Y-%m-%d"))
Spain %<>%
  mutate(Date= as.Date(Date, format= "%Y-%m-%d"))
Scotland %<>%
  mutate(Date= as.Date(Date, format= "%Y-%m-%d"))

Remote$Date[which(Remote$Date <= "2020-10-01")] <- NA

Remote<-na.omit(Remote)

hist(Remote$Date,"days")
hist(Remote$Solicitudes)
hist(Remote$Visualizaciones)
hist(WorldWide$Date,"days")
hist(UK$Date,"days")
hist(Spain$Date,"days")
hist(Scotland$Date,"days")

```