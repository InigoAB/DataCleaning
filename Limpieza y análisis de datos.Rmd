---
title: "PRACTICA 2: LIMPIEZA Y VALIDACIÓN DE LOS DATOS"
author: "Gines Molina e Iñigo Alvarez"
date: "29/12/2020"
output:
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r 1, message=FALSE, warning=FALSE}
# Carga de librerías
library(dplyr)
library(VIM)
library(gridExtra)
library(ggplot2)
library(stringi)
library(stringr)
library(tmaptools)
library(polycor)
library(reshape2)
library(wordcloud)
library(wordcloud2)
library(viridis)
# library(highcharter)
# library(maps)
# library(ggmap)
# library(ggpubr)
# library(tidyverse)
# library(plotly)
# library(magrittr)
```

# 1. Descripción del dataset

El dataset es un listado de ofertas de trabajo publicadas en Linkedin para ciencia de datos (término "Data Scientist") que fue obtenido en la primera parte de esta práctica.

Consta de 6 archivos CSV con las ofertas encontradas a nivel de Escocia, Reino Unido, España, Mundo y remoto.

```{r}
#TODO ¿Porque es importante y qué pregunta/problema pretende responder? AMPLIAR
```

# 2. Integración y selección de los datos de interés a analizar

```{r}
#TODO BREVE INTRODUCCIÓN
```

El primer paso es integrar los distintos archivos en un mismo dataset.

```{r}
Scotland <- read.csv("data/Scotland.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
Spain <- read.csv("data/Spain.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
UK <- read.csv("data/UK.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
WorldWide <- read.csv("data/WorldWide.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
Remote <- read.csv("data/Remote.csv", header=TRUE, sep=",", stringsAsFactors=FALSE)
Scotland$dataset <- "Scotland"
Spain$dataset <- "Spain"
UK$dataset <- "UK"
WorldWide$dataset <- "WorldWide"
Remote$dataset <- "Remote"
jobs <- rbind(Scotland, Spain, UK, WorldWide, Remote)
```

```{r}
str(jobs)
```

Quitamos la primera columna ya que no es más que un índice que no aporta ninguna información.

```{r}
jobs <- jobs[2:18]
```

```{r}
#TODO EXPLICACIÓN de las variables (COPIAR DE LA ANTERIOR PRACTICA)
```

# 3. Limpieza de los datos

Al haber integrado varios archivos de búsquedas de ámbitos territoriales que se solapan lo primero es eliminar las posibles duplicidades que podamos encontrar.

```{r}
sum(duplicated(jobs[1:5]))
```

Hay `r sum(duplicated(jobs[1:16]))` observaciones duplicadas así que las eliminamos.

```{r}

jobs_with_duplicates <- jobs

jobs <- jobs %>% 
  distinct(Job.ID, Company.Name, Location, .keep_all = TRUE)
```

## 3.1. Valores perdidos

Buscamos valores perdidos.

```{r}
sum(is.na(jobs))
sum(jobs=="")
sum(jobs=="None")
```

Sustituimos los valores en blanco por NA.

```{r}
jobs[jobs==""] <-- NA
jobs[jobs=="None"] <-- NA
jobs_with_duplicates[jobs_with_duplicates==""] <-- NA
jobs_with_duplicates[jobs_with_duplicates=="None"] <-- NA
```

Para corrgir los tipos de datos vamos a ver qué variables se podrían convertir a tipo factor.

```{r}
sapply(jobs, function(x) length(unique(x)))
```

Corregimos los tipos de los datos.

```{r}
jobs$Date <- as.Date(jobs$Date, format="%Y-%m-%d")
jobs$Solicitudes <- as.numeric(jobs$Solicitudes)
jobs$Visualizaciones <- as.numeric(jobs$Visualizaciones)
jobs$Level <- as.factor(jobs$Level)
jobs$Type <- as.factor(jobs$Type)
jobs$Empleados <- as.factor(jobs$Empleados)
jobs$Quick.Application <- as.factor(jobs$Quick.Application)
jobs$Recommended.Flavor <- as.factor(jobs$Recommended.Flavor)
jobs$dataset <- as.factor(jobs$dataset)

jobs_with_duplicates$Date <- as.Date(jobs_with_duplicates$Date, format="%Y-%m-%d")
jobs_with_duplicates$Solicitudes <- as.numeric(jobs_with_duplicates$Solicitudes)
jobs_with_duplicates$Visualizaciones <- as.numeric(jobs_with_duplicates$Visualizaciones)
jobs_with_duplicates$Level <- as.factor(jobs_with_duplicates$Level)
jobs_with_duplicates$Type <- as.factor(jobs_with_duplicates$Type)
jobs_with_duplicates$Empleados <- as.factor(jobs_with_duplicates$Empleados)
jobs_with_duplicates$Quick.Application <- as.factor(jobs_with_duplicates$Quick.Application)
jobs_with_duplicates$Recommended.Flavor <- as.factor(jobs_with_duplicates$Recommended.Flavor)
jobs_with_duplicates$dataset <- as.factor(jobs_with_duplicates$dataset)
```

Nos quedamos solo con las ofertas publicadas a partir de...

```{r}
#TODO "ESTO SERIA SOLO EN VISUALIZACIONES POR FECHA"
```

```{r}
jobs <- jobs %>% 
  filter(Date >= "2020-10-01")
```

```{r}
nrow(jobs)
```

Mostramos la distribución de los valores perdidos.

```{r warning=FALSE}
sum(is.na(jobs))
aggr(jobs, numbers=TRUE, sortVars=TRUE, labels=names(jobs),
cex.axis=.7, gap=3, ylab=c("Missing data","Pattern"))
```

```{r}
jobs <- jobs %>%
  group_by(Level, Quick.Application) %>%
    mutate(Solicitudes = ifelse(is.na(Solicitudes), median(Solicitudes, na.rm = TRUE), Solicitudes))
jobs <- jobs %>%
  group_by(Level, Quick.Application) %>%
    mutate(Visualizaciones = ifelse(is.na(Visualizaciones), median(Visualizaciones, na.rm = TRUE), Visualizaciones))
```

Comprobamos cómo quedan ahora los valores perdidos.

```{r warning=FALSE}
sum(is.na(jobs))
aggr(jobs, numbers=TRUE, sortVars=TRUE, labels=names(jobs),
cex.axis=.7, gap=3, ylab=c("Missing data","Pattern"))
```

Los valores perdidos se han reducido bastante y los que quedan se dejan así porque será importante para posteriores análisis.

```{r message=FALSE, warning=FALSE}

grid.arrange(
  qplot(Date, data=jobs)+ theme(axis.text.x = element_text(angle = 25)),
  qplot(Level, data=jobs)+ theme(axis.text.x = element_text(angle = 20, hjust=0.7)),
  qplot(Type, data=jobs)+ theme(axis.text.x = element_text(angle = 20, hjust=1)),
  qplot(Solicitudes, data=jobs),
  qplot(Empleados, data=jobs)+ theme(axis.text.x = element_text(angle = 20, hjust=1)),
  qplot(Visualizaciones, data=jobs),
  qplot(Recommended.Flavor, data=jobs)+ theme(axis.text.x = element_text(angle = 20, hjust=1))
)
```

Hacemos un boxplot para cada una de las variables numéricas.

```{r}
ggplot(jobs, aes(x=Level, y=Solicitudes, color=Level)) + 
  ggtitle("Diagrama de cajas") + 
  scale_color_brewer(palette="Dark2") +
  geom_boxplot() +
  theme(legend.position = "null") +
  geom_jitter(width = 0.1)

ggplot(jobs, aes(x=Level, y=Visualizaciones, color=Level)) + 
  ggtitle("Diagrama de cajas") + 
  scale_color_brewer(palette="Dark2") +
  geom_boxplot() +
  theme(legend.position = "null") +
  geom_jitter(width = 0.1)
```

```{r}
#TODO EXPLICAR RESULTADOS
```

```{r}
tail(sort(boxplot.stats(jobs$Solicitudes)$out),5)
```

Voy a observar las tres ofertas de trabajo que superan las 3000 solicitudes.

```{r}
#TODO ¿es necesario estos apartados de acontinuacion? Yo lo eliminaría, no aporta nada

# FALTARÍA PONERLO EXACTAMENTE COMO PONE EN EL ENUNCIADO DE LA PRÁCTICA. PUNTOS 3.1 Y 3.2
# PARA LOS VALORES EXTREMOS, LO HACEMOS PARA LAS FECHAS POR EJEMPLO. SE PUEDE COMENTAR AHORA
```

# 4. Análisis de los datos.

## 4.1. Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar)

```{r}
#TODO EXPLICAR
```

### 4.1.1 Preparación de los datos de Longitud y Latitud.

```{r}
#TODO EXPLICAR
```

```{r}
jobs$Location <- str_remove(jobs$Location, "^*y alrededores.*$")
jobs$Location[which(jobs$Location == "Greater Barcelona Metropolitan Area")] <- "Barcelona"
jobs$Location[which(jobs$Location == "Cracow, Lesser Poland District, Poland")] <- "Cracow, Poland"
jobs$Location[which(jobs$Location == "New York City Metropolitan Area")] <- "New York"
jobs$Location[which(jobs$Location == "Hong Kong, Hong Kong SAR")] <- "Hong Kong"
jobs$Location[which(jobs$Location == "Burnaby (Maywood / Marlborough / Oakalla / Windsor), V5H, CA")] <- "Burnaby"
jobs$Location[which(jobs$Location == "District Brno-City, Czech Republic")] <- "Czech Republic"
jobs$Location[which(jobs$Location == "Silkeborg, Middle Jutland, Denmark")] <- "Denmark"
jobs$Location[which(jobs$Location == "Prague, The Capital, Czech Republic")] <- "Prague"
jobs$Location[which(jobs$Location == "Kuala Lumpur, Federal Territory of Kuala Lumpur, Malaysia")] <- "Malaysia"
jobs$Location[which(jobs$Location == "Genève et périphérie")] <- "Genève"
jobs$Location[which(jobs$Location == "Dallas-Fort Worth Metroplex")] <- "Dallas"
jobs$Location[which(jobs$Location == "Raleigh-Durham-Chapel Hill Area")] <- "Raleigh"
jobs$Location[which(jobs$Location == "Herzliyya, Tel Aviv, Israel")] <- "Herzliya"
jobs$Location[which(jobs$Location == "New Territories, Hong Kong SAR")] <- "Hong Kong"
jobs$Location[which(jobs$Location == "Des Moines Metropolitan Area")] <- "Des Moines"
jobs$Location[which(jobs$Location == "Greater Minneapolis-St. Paul Area")] <- "Minneapolis"
jobs$Location[which(jobs$Location == "Greater Munich Metropolitan Area")] <- "Munich"
jobs$Location[which(jobs$Location == "Gurgaon Sub-District, Haryana, India")] <- "Gurgaon"
jobs$Location[which(jobs$Location == "Village of Mayfield, OH, US")] <- "Cleveland"
jobs$Location[which(jobs$Location == "Kraków i okolice")] <- "Kraków"

OSM_jobs <- geocode_OSM(jobs$Location, details = TRUE, as.data.frame = TRUE)
jobs_merged<-merge(x=jobs,y=OSM_jobs[2:3], by = 0, all= TRUE)

head(OSM_jobs)
head(jobs_merged)
```

```{r}
#TODO EXPLICAR RESULTADOS
```

## 4.2. Comprobación de la normalidad y homogeneidad de la varianza.

En el dataset se obtienen XX variables, XX categóricas y XX numéricas. En este apartado se busca verificar si las variables numéricas (lat, lon, Solicitudes y Visualizaciones) siguen una distribución normal.

Para verificarlo, primero se realizará un estudio visual de la normalidad mediante la gráfica quantile-quantile (Q-Q) que dibuja la correlación entre una muestra dada y la distribución normal. En esta primera gráfica se podrá observar el grado de aproximación o similitud con la línea de referencia de 45 grados. Además, también se realizará el estudio visual sobre el histograma de la variable

```{r}

par(mfrow=c(2,2))
for(i in 1:ncol(jobs_merged)) {
  if (is.numeric(jobs_merged[,i])){
    qqnorm(jobs_merged[,i],main = paste("Normal Q-Q Plot for ",colnames(jobs_merged)[i]))
    qqline(jobs_merged[,i],col="red")
    hist(jobs_merged[,i],
      main=paste("Histogram for ", colnames(jobs_merged)[i]),
      xlab=colnames(jobs_merged)[i], freq = FALSE)
  }
}

```

No parece que las gráficas representen que claramente las variables siguen una distribución normal. Para verificarlo, a continuación se realiza la comprobación mediante el test de Shapiro-Wilk y así no dar lugar a errores.

```{r}

for(i in 1:ncol(jobs_merged)) {
  if (is.numeric(jobs_merged[,i])){
    result <- shapiro.test(jobs_merged[,i])
    print(paste("Resultados para: ", colnames(jobs_merged)[i]))
    print(result)
  }
}
```

Se puede observar que a través de los test aplicados a las variables cuantitativas, el p-value es menor que 0.05, por lo que se deberá rechazar la hipótesis nula y aceptar que las variables no siguen una distribución normal. Por lo tanto, se deberán utlizar métodos para el análisis que no supongan que las variables cuantitativas siguen una distribución normal.

No obstante, cuando el número de observaciones es mayor o igual a 30, como es el caso de estas variables cuantitativas y debido al teorema central del límite, se podrán utilizar pruebas paramétricas asumiendo que con un aumento de observaciones, la distribución se volvería normal y en forma de campana. Así las variables se podrían aproximar como una distribución normal de media 0 y desviación estándar 1.

A continuación, se lleva a cabo la comprobación de homogeneidad de las varianzas. Se utilizará el test de Flinge-Killneen que resulta apropiado para variables no paramétricas que no siguen una distribución normal.

```{r}

fligner.test(Visualizaciones ~ lat, jobs_merged)
fligner.test(Visualizaciones ~ lon, jobs_merged)
fligner.test(Solicitudes ~ lat, jobs_merged)
fligner.test(Solicitudes ~ lon, jobs_merged)
fligner.test(Visualizaciones ~ Solicitudes, jobs_merged)

```

Se obtiene como resultado que las variables numéricas no son homogéneas según su varianza ya que en todos estos test se consigue un p-value menor de 0.05. Se deberá tener en cuenta para la aplicación de métodos analíticos que asuman homogeneidad de las varianzas.

## 4.3 Aplicación de pruebas estadísticas para comparar los grupos de datos.

En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc. Aplicar al menos tres métodos de análisis diferentes.

```{r}
#TODO EXPLICAR QUE SE LLEVARA A CABO
```

### 4.3.1 Correlaciones.

```{r}
#TODO INTRODUCCION
```

```{r}

borrar <- c("Date","Row.names")
jobs_corr <- jobs_merged[ , !(names(jobs_merged) %in% borrar)]

corr_matrix<-hetcor(jobs_corr, ML=FALSE, std.err=FALSE)
corr_matrix$correlations

ggplot(
  melt(corr_matrix$correlations),
  aes(Var2, Var1, fill = value)
)+
geom_tile(color = "white")+
scale_fill_gradient2(
  low = "blue",
  high = "red",
  mid = "white",
  midpoint = 0,
  limit = c(-1,1),
  space = "Lab",
  name="Correlation") +
theme_minimal()+ # minimal theme
theme(
  axis.text.x = element_text(
  angle = 45, vjust = 1,
  size = 12, hjust = 1))+
coord_fixed()

```

```{r}
#TODO EXPLICAR RESULTADOS
```

### 4.3.2 Modelo de regresión lineal

```{r}
#TODO EXPLICAR
```

```{r}

remote_lat_positive<-jobs_merged[which(jobs_merged$lat>0),]
ntrain <- nrow(remote_lat_positive)*0.8
ntest <- nrow(remote_lat_positive)*0.2
set.seed(12312)
index_train<-sample(1:nrow(remote_lat_positive),size = ntrain)
train<-remote_lat_positive[index_train,]
test<-remote_lat_positive[-index_train,]
modelo<-lm(formula = Solicitudes ~ lat + lon + Visualizaciones, data=train)
summary(modelo)

prob_sl<-predict(modelo, test, type="response")
mc_sl<-data.frame(
  real=test$Solicitudes,
  predicted= prob_sl,
  dif=ifelse(test$Solicitudes>prob_sl, -prob_sl*100/test$Solicitudes,prob_sl*100/test$Solicitudes)
  )
colnames(mc_sl)<-c("Real","Predecido","Dif%")
knitr::kable(mc_sl)

# Predicción

newdata <- data.frame(
  lat = 41.38,
  lon = 2.17,
  Visualizaciones = 37
)
# Predecir el número de solicitudes respecto a la latitud, longitud y el número de visualizaciones.
predict(modelo, newdata)

```

```{r}
#TODO EXPLICAR RESULTADOS
```

## 4.3.3 Modelo de Regresión logística

```{r}
#TODO EXPLICAR
```

```{r}
lgm <- glm(formula = Quick.Application ~ Solicitudes + Level + Recommended.Flavor, family = binomial(link=logit), data = jobs)
lgm
```

$${logit}(p_i)=\ln\left(\frac{p_i}{1-p_i}\right) = `r round(lgm_$coefficients[1],2)` `r round(lgm_$coefficients[2],2)`x_1 + `r round(lgm_$coefficients[3],2)`x_2 `r round(lgm_$coefficients[4],2)`x_3$$

```{r}
summary(jobs)
```

```{r}
head(jobs, 10)
```

```{r}
tail(jobs, 10)
```

```{r}
#TODO EXPLICAR RESULTADOS
```

# 5. Representación de los resultados a partir de las tablas y gráficas.

```{r}
#TODO EXPLICAR
```


```{r}
#TODO FALTA GRAFICAS Diferencias entre localizaciones. Entre latitudes positivas y negativas. Entre Remoto o no según cada localizacion. Mapas de localizaciones. WordCloud de localizaciones y tabla de frecuencias de palabras. Top industries grafica de industrias mas comunes segun localizacion.
jobs_merged %>% 
  group_by(dataset) %>% 
  count(Level) %>%
  mutate(freq = round(n / sum(n) * 100, 0)) %>% 
  ggplot(mapping = aes(y = n, x = dataset, color=Level, fill=Level)) + geom_bar( stat="identity") + geom_text(aes(label = paste(freq, "%")), color="black") + facet_wrap(~Level)+ theme(axis.text.x = element_text(angle = 55, hjust=1))

jobs_merged %>% 
  group_by(dataset) %>% 
  count(Type) %>%
  mutate(freq = round(n / sum(n) * 100, 0)) %>% 
  ggplot(mapping = aes(y = n, x = dataset, color=Type, fill=Type)) + geom_bar( stat="identity") + geom_text(aes(label = paste(freq, "%")), color="black") + facet_wrap(~Type)+ theme(axis.text.x = element_text(angle = 55, hjust=1))

jobs_merged %>% 
  group_by(dataset) %>% 
  count(Quick.Application) %>%
  mutate(freq = round(n / sum(n) * 100, 0)) %>% 
  ggplot(mapping = aes(y = n, x = dataset, color=Quick.Application, fill=Quick.Application)) + geom_bar( stat="identity") + geom_text(aes(label = paste(freq, "%")), color="black") + facet_wrap(~Quick.Application)+ theme(axis.text.x = element_text(angle = 55, hjust=1))

jobs_merged %>% 
  group_by(dataset) %>% 
  count(Empleados) %>%
  mutate(freq = round(n / sum(n) * 100, 0)) %>% 
  ggplot(mapping = aes(y = n, x = dataset, color=Empleados, fill=Empleados)) + geom_bar( stat="identity") + geom_text(aes(label = paste(freq, "%")), color="black") + facet_wrap(~Empleados)+ theme(axis.text.x = element_text(angle = 55, hjust=1))

```

## 5.1 Mapas por localización y a nivel global

```{r}
#TODO EXPLICAR
```

```{r}

NI_world <- map_data("world")

p_world <- jobs_merged %>%
  ggplot() +
    geom_polygon(data = NI_world, aes(x=long, y = lat, group = group), fill="grey", alpha=0.3) +
    geom_count(aes(x=lon, y=lat)) +
    scale_size_continuous(range=c(0.01,3)) +
    scale_color_viridis(option="inferno") +
    theme_void() +
    coord_map() +
    theme(legend.position = "none")
p_world

p_remote <- jobs_merged[which(jobs_merged$dataset=="Remote"),] %>%
  ggplot() +
    geom_polygon(data = NI_world, aes(x=long, y = lat, group = group), fill="grey", alpha=0.3) +
    geom_count(aes(x=lon, y=lat)) +
    scale_size_continuous(range=c(0.01,3)) +
    scale_color_viridis(option="inferno") +
    theme_void() +
    coord_map() +
    theme(legend.position = "none")
p_remote

NI_uk <- map_data("world") %>%
  filter(region == "UK")
p_uk <- jobs_merged[which(jobs_merged$dataset=="UK"),] %>%
  ggplot() +
    geom_polygon(data = NI_uk, aes(x=long, y = lat, group = group), fill="grey", alpha=0.3) +
    geom_count(aes(x=lon, y=lat)) +
    scale_size_continuous(range=c(1,7)) +
    scale_color_viridis(option="inferno") +
    theme_void() +
    coord_fixed(ratio = 1.3, 
              xlim = c(-10,3), 
              ylim = c(50, 59)) +
    theme(legend.position = "none")
p_uk

NI_scotland <- map_data("world") %>%
  filter(region == "UK")
p_scotland <- jobs_merged[which(jobs_merged$dataset=="Scotland"),] %>%
  ggplot() +
    geom_polygon(data = NI_scotland, aes(x=long, y = lat, group = group), fill="grey", alpha=0.3) +
    geom_count(aes(x=lon, y=lat)) +
    scale_size_continuous(range=c(1,7)) +
    scale_color_viridis(option="inferno") +
    theme_void() +
    coord_fixed(ratio = 1.3, 
              xlim = c(-10,3), 
              ylim = c(50, 59)) +
    theme(legend.position = "none")
p_scotland

NI_spain <- map_data("world") %>%
  filter(region == "Spain")
p_spain <- jobs_merged[which(jobs_merged$dataset=="Spain"),] %>%
  ggplot() +
    geom_polygon(data = NI_spain, aes(x=long, y = lat, group = group), fill="grey", alpha=0.3) +
    geom_count(aes(x=lon, y=lat)) +
    scale_size_continuous(range=c(1,15)) +
    scale_color_viridis(option="inferno") +
    theme_void() +
    ylim(35,45) +
    coord_map() +
    theme(legend.position = "none")
p_spain

```

```{r}
#TODO EXPLICAR RESULTADOS
```

## 5.2 Patrones de publicación de trabajos por fecha

```{r}

jobs_date_pattern <- jobs_merged

jobs_date_pattern <- jobs_date_pattern %>%
  filter(Date >= "2020-10-01")

nrow(jobs_date_pattern)

hist(jobs_date_pattern$Date,"days")

hist(jobs_date_pattern[which(jobs_date_pattern$dataset=="Spain"),"Date"],"days")
```

```{r}
#TODO EXPLICAR RESULTADOS
```

## 5.3 Nube de palabras más usadas en UK

```{r}
#TODO EXPLICAR
```

```{r}

UK$Description <- gsub(",", "", UK$Description, fixed = TRUE)
UK$Description <- gsub(".", "", UK$Description, fixed = TRUE)
UK$Description <- gsub("/", "", UK$Description, fixed = TRUE)

df1 <- data.frame(table(unlist(strsplit(tolower(UK$Description), " "))))

borrar <- c("and"," ","","to","the","of","a","in","with","for","you","our","is","as","we","on","or","are","be","will","an","your","work","working","that","this","have","at","their","role","from","skills","by","looking","what","–","can","all","across","they","who","such","new","more","if","through","&","any","when","well","us","uk","take","not","also","these","there","get","(eg","while","so","further","per","youll","using","based","including","within","where","has","when","able","it","other","it´s","than","we’re","we're","we’ve","we’ll","we've","we’d","style='font-weight:","we'll","we'd","(we","about","many","you’ll","you'll","you’re","you're","you?","you!","you’d","you’ve","you've","you'd","them","out","over","may","been","here","those","do","one","but","into","must","just","its","etc","some","no","should","every","-","then","come","each","*","	+")
df1 <- df1[!(df1$Var1 %in% borrar),]

set.seed(1234)

wordcloud(words = df1$Var1, freq = df1$Freq, min.freq = 1, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8,"Dark2"))

word_dataframe <- df1[order(-df1$Freq),]

wordcloud2(data=word_dataframe[2:400,], size=1.6, color='random-dark')

word_dataframe[1:25,]
```

Entre los lenguajes de programación, herramientas y plataformas/servicios cabe destacar estas coincidencias:

- R: 223 coincidencias
- SQL: 451 coincidencias
- AI: 308 coincidencias
- Python: 708 coincidencias
- BI: 136 coincidencias
- ETL: 183 coincidencias
- ML: 236 coincidencias
- Design: 881 coincidencias
- Azure: 225 coincidencias
- Cloud: 473 coincidencias
- AWS: 258 coincidencias

Lugares:
- London: 371 coincidencias
- home: 152 coincidencias
- remote: 176 coincidencias

```{r}
#TODO EXPLICAR RESULTADOS
```

# 6. Resolución del problema. Conclusiones.

